{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 1\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Wednesday, 20th of November 23:59h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet path similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reimplement the path similarity between two words in WordNet using the NLTK package. The path similarity between two words is given by the smallest similarity of any pair of their synsets.\n",
    "Obviously, do not use the pathsim function already implemented. From NLTK you should only use the synset/synsets/hyponym/hypernym functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import book\n",
    "from nltk.tree import Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4d33e5d7e529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mn_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mleavepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaf_treeposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "word_1 = wn.synset('good.n.1') \n",
    "word_2 = wn.synset('bag.n.1') \n",
    "Paths = word_1._shortest_hypernym_paths(word_2) \n",
    "Paths = {k:v for k,v in Paths.items() if v > 0} \n",
    "min(Paths, key=Paths.get) \n",
    "\n",
    "t = nltk.Tree.fromstring(word_1, word_2)\n",
    "n_leaves = len(t.leaves())\n",
    "leavepos = set(t.leaf_treeposition(n) for n in range(n_leaves))\n",
    "for pos in t.treepositions():\n",
    "        if pos not in leavepos:\n",
    "            print(t[pos].label(), len(pos))\n",
    "            \n",
    "# def pathSimilarity(word_1,word_2):\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you are provided a text as a string. Write a function that tokenizes the text and removes removes all the puntuation marks as well as lemmatizes it. The function should return a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-73e9c7a1100c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'austen-emma.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# def textNormalization(text):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "def textNormalization(text):\n",
    "    for token in tokenizer.tokenize(mystring):\n",
    "    kind, txt, val = token\n",
    "    if kind == tokenizer.TOK.WORD:\n",
    "        # Do something with word tokens\n",
    "        pass\n",
    "    else:\n",
    "        # Do something else\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you have a normalized text (obtained by applying the textNormalization function in the previous task.)\n",
    "In this task, we will implement Markov chain models of first and second order. The trained models should be stored in the following way:\n",
    "A first-order Markov chain model is given by a dictionary with a 2-tuples keys and floats as values. Each 2-tuple specifies the last word and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"apple\" was the previous word is 0.5\n",
    "\n",
    "A second-order Markov chain model is given by a dictionary with a 3-tuples keys and floats as values. Each 3-tuple specifies the two last words and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"the\", \"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"the\" and \"apple\" were the previous two words is 0.5 according to the model.\n",
    "Note: You can (and should) use defaultdict objects from the collections module with appropriate default values. Nonetheless, do not expect these data formats to scale well for large texts!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implement the following two functions that determines a maximum-likelihood first-order (second-order resp.) Markov model (in the previously specified format) from a text given as a string. As an optional argument also a Laplace correction can be specified. The correction is given by an integer number that is to be treated as the number of pseudo-observations.\n",
    "\n",
    "Furthermore, implement a function that computes the perplexity of a model (either first- or second order Markov chain models in the specified format) on a given text (string).\n",
    "\n",
    "Follow the following function headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_markov(training_text, laplace_correction = 0):\n",
    "    pass\n",
    "\n",
    "def second_order_markov(training_text, laplace_correction = 0):\n",
    "    pass\n",
    "\n",
    "def perplexity (evaluation_text, model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that implements the Viterbi algorithm as introduced in the lecture. The function takes as input a sentence, state transition probability and word emission probability and returns the sequence of POS tags. State transition and word emission probabilities are also provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-319acf2cb3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     initialisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmax_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_pro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mWord_emission_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "states = ['<s>','NNP','MD','VB','JJ','NN','RB','DT']\n",
    "\n",
    "Sentence = 'Janet will back the bill'\n",
    "Sentence = Sentence.split()\n",
    "\n",
    "start_pro = [0, 0.2767,0.006,0.0031,0.0453,0.0449,0.0510,0.2026]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def Viterbi(State_trans_prob, Word_emission_prob,Sentence):\n",
    "    max_p = np.zeros((len(Sentence), len(states)))\n",
    "    path = np.zeros((len(states), len(Sentence)))\n",
    "    \n",
    "#     initialisation \n",
    "for i in range(len(states)): \n",
    "        max_p[0][i] = start_pro[i] * Word_emission_prob[i][Sentence[0]]\n",
    "        path[i][0] = i\n",
    "for t in range(1, len(Sentence)):\n",
    "    newpath = np.zeros((len(states), len(Sentence)))\n",
    "    \n",
    "    for y in range(len(states)):\n",
    "        prob = -1\n",
    "        \n",
    "        for y0 in range(len(states)):\n",
    "            nprob = max_p[t-1][y0] * State_trans_prob[y0][y] * Word_emission_prob[y][Sentence[t]]\n",
    "            \n",
    "            if nprob > prob:\n",
    "                    prob = nprob\n",
    "                    state = y0\n",
    "                    #   记录路径\n",
    "                    max_p[t][y] = prob\n",
    "                    \n",
    "                    for m in range(t):\n",
    "                        newpath[y][m] = path[state][m]\n",
    "                    newpath[y][t] = y\n",
    "                    \n",
    "max_prob = -1\n",
    "path_state = 0\n",
    "\n",
    "for y in range(len(states)):\n",
    "    \n",
    "    if max_p[len(Sentence)-1][y] > max_prob:\n",
    "            max_prob = max_p[len(Sentence)-1][y]\n",
    "            path_state = y\n",
    "\n",
    "    return path[path_state]   \n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({('<s>', 'NNP'): 0.2767, ('<s>', 'MD'): 0.006, ('<s>', 'VB'): 0.0031, ('<s>', 'JJ'): 0.0453, ('<s>', 'NN'): 0.0449, ('<s>', 'RB'): 0.051, ('<s>', 'DT'): 0.2026, ('NNP', 'NNP'): 0.3777, ('NNP', 'MD'): 0.011, ('NNP', 'VB'): 0.0009, ('NNP', 'JJ'): 0.0084, ('NNP', 'NN'): 0.0584, ('NNP', 'RB'): 0.0514, ('NNP', 'DT'): 0.2231, ('MD', 'NNP'): 0.0008, ('MD', 'MD'): 0.0002, ('MD', 'VB'): 0.7968, ('MD', 'JJ'): 0.0005, ('MD', 'NN'): 0.0008, ('MD', 'RB'): 0.1698, ('MD', 'DT'): 0.0041, ('VB', 'NNP'): 0.0322, ('VB', 'MD'): 0.0005, ('VB', 'VB'): 0.005, ('VB', 'JJ'): 0.0837, ('VB', 'NN'): 0.0615, ('JJ', 'NNP'): 0.0366, ('JJ', 'MD'): 0.0004, ('JJ', 'VB'): 0.0001, ('JJ', 'JJ'): 0.0733, ('JJ', 'NN'): 0.4509, ('JJ', 'RB'): 0.0036, ('JJ', 'DT'): 0.0036, ('NN', 'NNP'): 0.0096, ('NN', 'MD'): 0.0176, ('NN', 'VB'): 0.0014, ('NN', 'JJ'): 0.0086, ('NN', 'NN'): 0.1216, ('NN', 'RB'): 0.0177, ('NN', 'DT'): 0.0068, ('RB', 'NNP'): 0.0068, ('RB', 'MD'): 0.0102, ('RB', 'VB'): 0.1011, ('RB', 'JJ'): 0.1012, ('RB', 'NN'): 0.012, ('RB', 'RB'): 0.0728, ('RB', 'DT'): 0.0479, ('DT', 'NNP'): 0.1147, ('DT', 'MD'): 0.0021, ('DT', 'VB'): 0.0002, ('DT', 'JJ'): 0.2157, ('DT', 'NN'): 0.4744, ('DT', 'RB'): 0.0102, ('DT', 'DT'): 0.0017},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "State_trans_prob = {('<s>','NNP'):0.2767,('<s>','MD'):0.006,('<s>','VB'):0.0031,('<s>','JJ'):0.0453,('<s>','NN'):0.0449,\n",
    "                   ('<s>','RB'):0.0510,('<s>','DT'):0.2026,\n",
    "                   ('NNP','NNP'):0.3777,('NNP','MD'):0.0110,('NNP','VB'):0.0009,('NNP','JJ'):0.0084,('NNP','NN'):0.0584,\n",
    "                   ('NNP','RB'):0.0090,('NNP','DT'):0.0025,\n",
    "                   ('MD','NNP'):0.0008,('MD','MD'):0.0002,('MD','VB'):0.7968,('MD','JJ'):0.0005,('MD','NN'):0.0008,\n",
    "                   ('MD','RB'):0.1698,('MD','DT'):0.0041,\n",
    "                   ('VB','NNP'):0.0322,('VB','MD'):0.0005,('VB','VB'):0.0050,('VB','JJ'):0.0837,('VB','NN'):0.0615,\n",
    "                   ('NNP','RB'):0.0514,('NNP','DT'):0.2231,\n",
    "                   ('JJ','NNP'):0.0366,('JJ','MD'):0.0004,('JJ','VB'):0.0001,('JJ','JJ'):0.0733,('JJ','NN'):0.4509,\n",
    "                   ('JJ','RB'):0.0036,('JJ','DT'):0.0036,\n",
    "                   ('NN','NNP'):0.0096,('NN','MD'):0.0176,('NN','VB'):0.0014,('NN','JJ'):0.0086,('NN','NN'):0.1216,\n",
    "                   ('NN','RB'):0.0177,('NN','DT'):0.0068,\n",
    "                   ('RB','NNP'):0.0068,('RB','MD'):0.0102,('RB','VB'):0.1011,('RB','JJ'):0.1012,('RB','NN'):0.0120,\n",
    "                   ('RB','RB'):0.0728,('RB','DT'):0.0479,\n",
    "                   ('DT','NNP'):0.1147,('DT','MD'):0.0021,('DT','VB'):0.0002,('DT','JJ'):0.2157,('DT','NN'):0.4744,\n",
    "                   ('DT','RB'):0.0102,('DT','DT'):0.0017}\n",
    "# State_trans_prob = np.array(State_trans_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Janet', 'NNP'): 3.2e-05, ('will', 'MD'): 0.308431, ('will', 'VB'): 2.8e-05, ('will', 'NN'): 0.0002, ('back', 'VB'): 0.000672, ('back', 'JJ'): 0.00034, ('back', 'NN'): 0.000223, ('back', 'RB'): 0.010446, ('the', 'NNP'): 4.8e-05, ('the', 'DT'): 0.506099, ('bill', 'VB'): 2.8e-05, ('bill', 'NN'): 0.002337}\n"
     ]
    }
   ],
   "source": [
    "Word_emission_prob = {('Janet','NNP'):0.000032, ('will','MD'):0.308431,('will','VB'):0.000028,('will','NN'):0.0002,\n",
    "                     ('back','VB'):0.000672,('back','JJ'):0.00034,('back','NN'):0.000223,('back','RB'):0.010446,\n",
    "                     ('the','NNP'):0.000048,('the','DT'):0.506099,('bill','VB'):0.000028,('bill','NN'):0.002337}\n",
    "# assume the missing entries are 0\n",
    "# Word_emission_prob = np.array(Word_emission_prob)\n",
    "# print(Word_emission_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Viterbi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-40bbb0d36e84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Janet will back the bill'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mViterbi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mState_trans_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord_emission_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Viterbi' is not defined"
     ]
    }
   ],
   "source": [
    "state_s = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "obser = [0, 1]\n",
    "Sentence = 'Janet will back the bill'\n",
    "Sentence = Sentence.split()\n",
    "Viterbi(State_trans_prob, Word_emission_prob, Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging with nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that utilizes the POS tagger in nltk to obtain the POS tags of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(sentence):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "posTag(Sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
