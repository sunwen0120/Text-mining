{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 1\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Wednesday, 20th of November 23:59h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet path similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reimplement the path similarity between two words in WordNet using the NLTK package. The path similarity between two words is given by the smallest similarity of any pair of their synsets.\n",
    "Obviously, do not use the pathsim function already implemented. From NLTK you should only use the synset/synsets/hyponym/hypernym functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathSimilarity(word_1,word_2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you are provided a text as a string. Write a function that tokenizes the text and removes removes all the puntuation marks as well as lemmatizes it. The function should return a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textNormalization(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you have a normalized text (obtained by applying the textNormalization function in the previous task.)\n",
    "In this task, we will implement Markov chain models of first and second order. The trained models should be stored in the following way:\n",
    "A first-order Markov chain model is given by a dictionary with a 2-tuples keys and floats as values. Each 2-tuple specifies the last word and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"apple\" was the previous word is 0.5\n",
    "\n",
    "A second-order Markov chain model is given by a dictionary with a 3-tuples keys and floats as values. Each 3-tuple specifies the two last words and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"the\", \"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"the\" and \"apple\" were the previous two words is 0.5 according to the model.\n",
    "Note: You can (and should) use defaultdict objects from the collections module with appropriate default values. Nonetheless, do not expect these data formats to scale well for large texts!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implement the following two functions that determines a maximum-likelihood first-order (second-order resp.) Markov model (in the previously specified format) from a text given as a string. As an optional argument also a Laplace correction can be specified. The correction is given by an integer number that is to be treated as the number of pseudo-observations.\n",
    "\n",
    "Furthermore, implement a function that computes the perplexity of a model (either first- or second order Markov chain models in the specified format) on a given text (string).\n",
    "\n",
    "Follow the following function headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_order_markov(training_text, laplace_correction = 0):\n",
    "    pass\n",
    "\n",
    "def second_order_markov(training_text, laplace_correction = 0):\n",
    "    pass\n",
    "\n",
    "def perplexity (evaluation_text, model):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that implements the Viterbi algorithm as introduced in the lecture. The function takes as input a sentence, state transition probability and word emission probability and returns the sequence of POS tags. State transition and word emission probabilities are also provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Viterbi(State_trans_prob, Word_emission_prob,sentence):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_trans_prob = {('<s>','NNP'):0.2767,('<s>','MD'):0.006,('<s>','VB'):0.0031,('<s>','JJ'):0.0453,('<s>','NN'):0.0449,\n",
    "                   ('<s>','RB'):0.0510,('<s>','DT'):0.2026,\n",
    "                   ('NNP','NNP'):0.3777,('NNP','MD'):0.0110,('NNP','VB'):0.0009,('NNP','JJ'):0.0084,('NNP','NN'):0.0584,\n",
    "                   ('NNP','RB'):0.0090,('NNP','DT'):0.0025,\n",
    "                   ('MD','NNP'):0.0008,('MD','MD'):0.0002,('MD','VB'):0.7968,('MD','JJ'):0.0005,('MD','NN'):0.0008,\n",
    "                   ('MD','RB'):0.1698,('MD','DT'):0.0041,\n",
    "                   ('VB','NNP'):0.0322,('VB','MD'):0.0005,('VB','VB'):0.0050,('VB','JJ'):0.0837,('VB','NN'):0.0615,\n",
    "                   ('NNP','RB'):0.0514,('NNP','DT'):0.2231,\n",
    "                   ('JJ','NNP'):0.0366,('JJ','MD'):0.0004,('JJ','VB'):0.0001,('JJ','JJ'):0.0733,('JJ','NN'):0.4509,\n",
    "                   ('JJ','RB'):0.0036,('JJ','DT'):0.0036,\n",
    "                   ('NN','NNP'):0.0096,('NN','MD'):0.0176,('NN','VB'):0.0014,('NN','JJ'):0.0086,('NN','NN'):0.1216,\n",
    "                   ('NN','RB'):0.0177,('NN','DT'):0.0068,\n",
    "                   ('RB','NNP'):0.0068,('RB','MD'):0.0102,('RB','VB'):0.1011,('RB','JJ'):0.1012,('RB','NN'):0.0120,\n",
    "                   ('RB','RB'):0.0728,('RB','DT'):0.0479,\n",
    "                   ('DT','NNP'):0.1147,('DT','MD'):0.0021,('DT','VB'):0.0002,('DT','JJ'):0.2157,('DT','NN'):0.4744,\n",
    "                   ('DT','RB'):0.0102,('DT','DT'):0.0017}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_emission_prob = {('Janet','NNP'):0.000032, ('will','MD'):0.308431,('will','VB'):0.000028,('will','NN'):0.0002,\n",
    "                     ('back','VB'):0.000672,('back','JJ'):0.00034,('back','NN'):0.000223,('back','RB'):0.010446,\n",
    "                     ('the','NNP'):0.000048,('the','DT'):0.506099,('bill','VB'):0.000028,('bill','NN'):0.002337}\n",
    "# assume the missing entries are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "Viterbi(State_trans_prob, Word_emission_prob, Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging with nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that utilizes the POS tagger in nltk to obtain the POS tags of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posTag(sentence):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "posTag(Sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
