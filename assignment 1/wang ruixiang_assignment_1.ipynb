{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 1\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Wednesday, 20th of November 23:59h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet path similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reimplement the path similarity between two words in WordNet using the NLTK package. The path similarity between two words is given by the smallest similarity of any pair of their synsets.\n",
    "Obviously, do not use the pathsim function already implemented. From NLTK you should only use the synset/synsets/hyponym/hypernym functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.2\n",
      "0.5\n",
      "0.25\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import deque\n",
    "\n",
    "def find_all_hypernyms(synset):\n",
    "    # construct a quese,which stores a list of key-value(hypernym-depth)\n",
    "    queue = deque([(synset, 0)]) \n",
    "    path = {} \n",
    "    while queue: # find all hypernyms using breadth-first search\n",
    "        s, depth = queue.popleft()\n",
    "        if s in path: \n",
    "        # if the hypernym already exists in the path{}, then ingonre it (beacause we wan to find the samllest Depth)\n",
    "            continue\n",
    "        # s(hypernyms) is the key in path{}, depth is the value in path{}\n",
    "        path[s] = depth\n",
    "        depth = depth + 1\n",
    "        queue.extend((hyper, depth) for hyper in s._hypernyms())\n",
    "    return path\n",
    "\n",
    "def compute_simpath(path_synset_1, path_synset_2):\n",
    "    for key1 in path_synset_1:\n",
    "        for key2 in path_synset_2:\n",
    "            # compare two dictionaries, find the lowest common hypernyms of these two synset\n",
    "            if key1 == key2 :  # shortest path\n",
    "                # compute the minmal path_length between these two synset\n",
    "                path_length = path_synset_1[key1] + path_synset_2[key2]\n",
    "                return 1 / (path_length + 1) # compute sim_path\n",
    "    return 0  # if don't have common hypernyms, path_length between two synsets is infinity, sim_path is 0\n",
    "\n",
    "def pathSimilarity(word_1,word_2):\n",
    "    simpath_list = [] # stores sim_path of any pair of synsets\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    synsets_2 = wn.synsets(word_2)\n",
    "    for synset1 in synsets_1:\n",
    "        for synset2 in synsets_2:\n",
    "            path_1 = find_all_hypernyms(synset1)\n",
    "            path_2 = find_all_hypernyms(synset2)\n",
    "            #print(compute_simpath(hypernyms_1,hypernyms_2))\n",
    "            simpath_list.append(compute_simpath(path_1, path_2))\n",
    "    # print(simpath_list)\n",
    "    # find the maximal sim_path, which represents the similarity of two words\n",
    "    word_simlarity = max (simpath_list)\n",
    "    return word_simlarity\n",
    "\n",
    "#test cases\n",
    "print(pathSimilarity('dog','dog'))\n",
    "print(pathSimilarity('dog','cat'))\n",
    "print(pathSimilarity('fund','budget'))\n",
    "print(pathSimilarity('bank','beach'))\n",
    "print(pathSimilarity('people','dog'))\n",
    "# synset_1.path_similarity(synset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you are provided a text as a string. Write a function that tokenizes the text and removes removes all the puntuation marks as well as lemmatizes it. The function should return a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def textNormalization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize (text)\n",
    "    for i,word in enumerate (words):\n",
    "        #consider number or number-word combination as a real word\n",
    "        if re.match('[a-z]+|[A-Z]+|[0-9]+',word):# use regex to judge if the word is a puntuation\n",
    "            # lemmatize the word. eg: words=>word \n",
    "            words[i] = lemmatizer.lemmatize(word)\n",
    "        else: # if word is a puntuation,remove it from list\n",
    "            words.remove(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenize', 'this', 'sentence', 'into', 'sentence', 'and', 'word', 'This', 'is', 'the', 'task', 'Mr.', 'Lemmerich', 'gave', 'to', 'you', 'Why', 'i.e.', 'what', 'the', 'purpose', 'of', 'this', 'is', 'remains', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "#test case\n",
    "text = \"Tokenize this sentence into sentences and words. This is the task Mr. Lemmerich gave to you. Why, i.e., what the purpose of this, is remains unknown.\"\n",
    "print(textNormalization(text))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you have a normalized text (obtained by applying the textNormalization function in the previous task.)\n",
    "In this task, we will implement Markov chain models of first and second order. The trained models should be stored in the following way:\n",
    "A first-order Markov chain model is given by a dictionary with a 2-tuples keys and floats as values. Each 2-tuple specifies the last word and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"apple\" was the previous word is 0.5\n",
    "\n",
    "A second-order Markov chain model is given by a dictionary with a 3-tuples keys and floats as values. Each 3-tuple specifies the two last words and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"the\", \"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"the\" and \"apple\" were the previous two words is 0.5 according to the model.\n",
    "Note: You can (and should) use defaultdict objects from the collections module with appropriate default values. Nonetheless, do not expect these data formats to scale well for large texts!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implement the following two functions that determines a maximum-likelihood first-order (second-order resp.) Markov model (in the previously specified format) from a text given as a string. As an optional argument also a Laplace correction can be specified. The correction is given by an integer number that is to be treated as the number of pseudo-observations.\n",
    "\n",
    "Furthermore, implement a function that computes the perplexity of a model (either first- or second order Markov chain models in the specified format) on a given text (string).\n",
    "\n",
    "Follow the following function headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function first_order_markov.<locals>.<lambda> at 0x00000205B48CBAF8>, {('The', 'web'): 0.0425531914893617, ('The', 'main'): 0.0425531914893617, ('web', 'can'): 0.06382978723404255, ('can', 'be'): 0.06382978723404255, ('be', 'thought'): 0.06382978723404255, ('thought', 'of'): 0.06382978723404255, ('of', 'a'): 0.05660377358490566, ('of', 'unannotated'): 0.05660377358490566, ('of', 'searching'): 0.03773584905660377, ('of', 'text'): 0.03773584905660377, ('of', 'search'): 0.03773584905660377, ('of', 'document'): 0.03773584905660377, ('a', 'a'): 0.06, ('a', 'huge'): 0.06, ('a', 'large'): 0.04, ('huge', 'corpus'): 0.06382978723404255, ('corpus', 'of'): 0.06382978723404255, ('unannotated', 'text.The'): 0.0425531914893617, ('unannotated', 'text'): 0.0425531914893617, ('text.The', 'web'): 0.043478260869565216, ('text', 'Web'): 0.0425531914893617, ('text', 'for'): 0.0425531914893617, ('Web', 'search'): 0.043478260869565216, ('search', 'engine'): 0.06382978723404255, ('engine', 'provide'): 0.0425531914893617, ('engine', 'is'): 0.0425531914893617, ('provide', 'an'): 0.043478260869565216, ('an', 'efficient'): 0.043478260869565216, ('efficient', 'mean'): 0.043478260869565216, ('mean', 'of'): 0.043478260869565216, ('searching', 'this'): 0.0425531914893617, ('searching', 'such'): 0.0425531914893617, ('this', 'large'): 0.043478260869565216, ('large', 'quantity'): 0.0425531914893617, ('large', 'set'): 0.0425531914893617, ('quantity', 'of'): 0.043478260869565216, ('for', 'relevant'): 0.043478260869565216, ('relevant', 'linguistic'): 0.043478260869565216, ('linguistic', 'example'): 0.0425531914893617, ('linguistic', 'pattern'): 0.0425531914893617, ('example', 'The'): 0.043478260869565216, ('main', 'advantage'): 0.043478260869565216, ('advantage', 'of'): 0.043478260869565216, ('is', 'size'): 0.043478260869565216, ('size', 'since'): 0.043478260869565216, ('since', 'you'): 0.043478260869565216, ('you', 'are'): 0.08333333333333333, ('are', 'searching'): 0.041666666666666664, ('are', 'more'): 0.041666666666666664, ('are', 'interested'): 0.041666666666666664, ('such', 'a'): 0.043478260869565216, ('set', 'of'): 0.043478260869565216, ('document', 'you'): 0.043478260869565216, ('more', 'likely'): 0.043478260869565216, ('likely', 'to'): 0.043478260869565216, ('to', 'find'): 0.043478260869565216, ('find', 'any'): 0.043478260869565216, ('any', 'linguistic'): 0.043478260869565216, ('pattern', 'you'): 0.043478260869565216, ('interested', 'in'): 0.043478260869565216}) \n",
      "\n",
      "defaultdict(<function second_order_markov.<locals>.<lambda> at 0x00000205B9384708>, {('The', 'web', 'can'): 0.03278688524590164, ('web', 'can', 'be'): 0.04838709677419355, ('can', 'be', 'thought'): 0.04838709677419355, ('be', 'thought', 'of'): 0.04838709677419355, ('thought', 'of', 'a'): 0.04838709677419355, ('of', 'a', 'a'): 0.04838709677419355, ('a', 'a', 'huge'): 0.04838709677419355, ('a', 'huge', 'corpus'): 0.04838709677419355, ('huge', 'corpus', 'of'): 0.04838709677419355, ('corpus', 'of', 'unannotated'): 0.04838709677419355, ('of', 'unannotated', 'text.The'): 0.03225806451612903, ('of', 'unannotated', 'text'): 0.03225806451612903, ('unannotated', 'text.The', 'web'): 0.03278688524590164, ('text.The', 'web', 'can'): 0.03278688524590164, ('unannotated', 'text', 'Web'): 0.03278688524590164, ('text', 'Web', 'search'): 0.03278688524590164, ('Web', 'search', 'engine'): 0.03278688524590164, ('search', 'engine', 'provide'): 0.03225806451612903, ('search', 'engine', 'is'): 0.03225806451612903, ('engine', 'provide', 'an'): 0.03278688524590164, ('provide', 'an', 'efficient'): 0.03278688524590164, ('an', 'efficient', 'mean'): 0.03278688524590164, ('efficient', 'mean', 'of'): 0.03278688524590164, ('mean', 'of', 'searching'): 0.03278688524590164, ('of', 'searching', 'this'): 0.03278688524590164, ('searching', 'this', 'large'): 0.03278688524590164, ('this', 'large', 'quantity'): 0.03278688524590164, ('large', 'quantity', 'of'): 0.03278688524590164, ('quantity', 'of', 'text'): 0.03278688524590164, ('of', 'text', 'for'): 0.03278688524590164, ('text', 'for', 'relevant'): 0.03278688524590164, ('for', 'relevant', 'linguistic'): 0.03278688524590164, ('relevant', 'linguistic', 'example'): 0.03278688524590164, ('linguistic', 'example', 'The'): 0.03278688524590164, ('example', 'The', 'main'): 0.03278688524590164, ('The', 'main', 'advantage'): 0.03278688524590164, ('main', 'advantage', 'of'): 0.03278688524590164, ('advantage', 'of', 'search'): 0.03278688524590164, ('of', 'search', 'engine'): 0.03278688524590164, ('engine', 'is', 'size'): 0.03278688524590164, ('is', 'size', 'since'): 0.03278688524590164, ('size', 'since', 'you'): 0.03278688524590164, ('since', 'you', 'are'): 0.03278688524590164, ('you', 'are', 'searching'): 0.031746031746031744, ('you', 'are', 'more'): 0.031746031746031744, ('you', 'are', 'interested'): 0.031746031746031744, ('are', 'searching', 'such'): 0.03278688524590164, ('searching', 'such', 'a'): 0.03278688524590164, ('such', 'a', 'large'): 0.03278688524590164, ('a', 'large', 'set'): 0.03278688524590164, ('large', 'set', 'of'): 0.03278688524590164, ('set', 'of', 'document'): 0.03278688524590164, ('of', 'document', 'you'): 0.03278688524590164, ('document', 'you', 'are'): 0.03278688524590164, ('are', 'more', 'likely'): 0.03278688524590164, ('more', 'likely', 'to'): 0.03278688524590164, ('likely', 'to', 'find'): 0.03278688524590164, ('to', 'find', 'any'): 0.03278688524590164, ('find', 'any', 'linguistic'): 0.03278688524590164, ('any', 'linguistic', 'pattern'): 0.03278688524590164, ('linguistic', 'pattern', 'you'): 0.03278688524590164, ('pattern', 'you', 'are'): 0.03278688524590164, ('are', 'interested', 'in'): 0.03278688524590164}) \n",
      "\n",
      "17.278266677332805\n",
      "999.9999999999997\n",
      "45.48833361466665\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "def first_order_markov(training_text, laplace_correction = 1): #laplace add-1 estimation\n",
    "    prob_dict = defaultdict( lambda: 0.001 ) # set inital value 0.001\n",
    "    words_of_text = textNormalization(training_text)# compute frequency of each word\n",
    "    V = len(set(words_of_text)) # Vocabulary size\n",
    "    words_freq = nltk.FreqDist(words_of_text)# dictionary: key = word, value = frequency\n",
    "    bi_grams = list(nltk.bigrams(words_of_text))# compute frequency of each bigrams\n",
    "    bi_grams_freq = nltk.FreqDist(bi_grams) # ditionary : key = bigrams(), value = frequency\n",
    "    # comute the condition probaality of last word and next word\n",
    "    for word in words_freq.keys():\n",
    "        for (last_word, next_word) in bi_grams_freq.keys():\n",
    "            if word == last_word :\n",
    "                # add-1 estimation\n",
    "                prob = (bi_grams_freq[(last_word, next_word)] + 1) / (words_freq[word] + V) \n",
    "                if (last_word, next_word) not in prob_dict.keys():\n",
    "                    prob_dict[(last_word, next_word)] = prob                   \n",
    "# this part of code is to compute the conditional probality of word follwed by itself,which doesn't exist in Bigarms\n",
    "#             elif word != last_word and word != next_word:\n",
    "#                 prob = 1/(words_freq[word] + V)\n",
    "#                 if (word, word) not in prob_dict.keys():\n",
    "#                     prob_dict[(word,word)] = prob\n",
    "    return prob_dict\n",
    "\n",
    "def second_order_markov(training_text, laplace_correction = 1): # laplace add-1 estimation\n",
    "    words_of_text = textNormalization(training_text)\n",
    "    prob_dict = defaultdict( lambda: 0.001 ) # set inital value 0.001\n",
    "    bi_grams = list(nltk.bigrams(words_of_text))\n",
    "    bi_grams_freq = nltk.FreqDist(bi_grams) # ditionary : key = bigrams(), value = frequency\n",
    "    V = len(set(bi_grams)) # vocablary size of bigrams\n",
    "    tri_grams = list(nltk.ngrams(words_of_text, 3))\n",
    "    tri_grams_freq = nltk.FreqDist(tri_grams)# ditionary : key = trigrams(), value = frequency\n",
    "    # compute conditional probability\n",
    "    for (last_word_1, next_word) in bi_grams_freq.keys():\n",
    "        for (last_1, last_2, next_1) in tri_grams_freq.keys():\n",
    "            if last_word_1 == last_1 and next_word == last_2:\n",
    "                prob = (tri_grams_freq[(last_1, last_2, next_1)] + 1)/(bi_grams_freq[(last_word_1, next_word)] + V ) \n",
    "                if (last_1, last_2, next_1) not in prob_dict:\n",
    "                    prob_dict[last_1, last_2, next_1] = prob\n",
    "    return prob_dict\n",
    "\n",
    "# perplexity of  first_order_markov model\n",
    "model = first_order_markov(text)\n",
    "def perplexity (evaluation_text, model): \n",
    "    test_set = textNormalization(evaluation_text)\n",
    "    bi_grams_test = list(nltk.bigrams(test_set))\n",
    "    perplexity = 1\n",
    "    N = 0\n",
    "    for (last_word, next_word) in bi_grams_test:\n",
    "        N = N + 1\n",
    "        perplexity = perplexity * (1 / model[(last_word, next_word)])\n",
    "    perplexity = pow (perplexity, 1 / float(N))\n",
    "    return perplexity\n",
    "\n",
    "# test cases\n",
    "text  = \"The web can be thought of as a huge corpus of unannotated text.The web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large set of documents, you are more likely to find any linguistic pattern you are interested in.\"\n",
    "print(first_order_markov(text), '\\n')     \n",
    "print(second_order_markov(text), '\\n')\n",
    "evaluation_text_1 = 'The web can be thought of as a huge corpus of unannotated text.'\n",
    "evaluation_text_2 = 'This is a Web.'\n",
    "evaluation_text_3 = 'Web search engines provide an apple.'\n",
    "print(perplexity(evaluation_text_1, model))\n",
    "print(perplexity(evaluation_text_2, model))\n",
    "print(perplexity(evaluation_text_3, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that implements the Viterbi algorithm as introduced in the lecture. The function takes as input a sentence, state transition probability and word emission probability and returns the sequence of POS tags. State transition and word emission probabilities are also provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def Viterbi(State_trans_prob, Word_emission_prob, sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    word_tag = {} # {'word':'tag'}\n",
    "    word_tag_prob = {{}} #{'word':{'tag':prob}}\n",
    "    \n",
    "    # initialization, find the tag of first element in words\n",
    "    first_tag = {}\n",
    "    for (word, tag), val in Word_emission_prob.items():\n",
    "        if words == word[0]:\n",
    "            prob = val * State_trans_prob[('<s>',tag)]\n",
    "            first_tag[tag] = prob\n",
    "    first_prob = max(first_tag.values())\n",
    "    for k,v in first_tag.items():\n",
    "        if v == max_prob:\n",
    "            word_tag[words[0]] = k\n",
    "            word_tag_prob[word[0]][k] = v\n",
    "    \n",
    "    # select nodes of each level, constuct path \n",
    "    probability = first_prob # init probailty before first iteration\n",
    "    word_tags = defaultdict(list)\n",
    "    for i in range(len(words)-1):\n",
    "      for (w, tag),v in Word_emission_prob.items():\n",
    "            if w == word[i+1]:\n",
    "                word_tags[w].append(tag)\n",
    "         \n",
    "#             for (tag_1,tag_2) in State_trans_prob.items(): \n",
    "#                 if word[i+1] == w and \n",
    "#                     proba = probability * v * \n",
    "                \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_trans_prob = {('<s>','NNP'):0.2767,('<s>','MD'):0.006,('<s>','VB'):0.0031,('<s>','JJ'):0.0453,('<s>','NN'):0.0449,\n",
    "                   ('<s>','RB'):0.0510,('<s>','DT'):0.2026,\n",
    "                   ('NNP','NNP'):0.3777,('NNP','MD'):0.0110,('NNP','VB'):0.0009,('NNP','JJ'):0.0084,('NNP','NN'):0.0584,\n",
    "                   ('NNP','RB'):0.0090,('NNP','DT'):0.0025,\n",
    "                   ('MD','NNP'):0.0008,('MD','MD'):0.0002,('MD','VB'):0.7968,('MD','JJ'):0.0005,('MD','NN'):0.0008,\n",
    "                   ('MD','RB'):0.1698,('MD','DT'):0.0041,\n",
    "                   ('VB','NNP'):0.0322,('VB','MD'):0.0005,('VB','VB'):0.0050,('VB','JJ'):0.0837,('VB','NN'):0.0615,\n",
    "                   ('VB','RB'):0.0514,('VB','DT'):0.2231,\n",
    "                   ('JJ','NNP'):0.0366,('JJ','MD'):0.0004,('JJ','VB'):0.0001,('JJ','JJ'):0.0733,('JJ','NN'):0.4509,\n",
    "                   ('JJ','RB'):0.0036,('JJ','DT'):0.0036,\n",
    "                   ('NN','NNP'):0.0096,('NN','MD'):0.0176,('NN','VB'):0.0014,('NN','JJ'):0.0086,('NN','NN'):0.1216,\n",
    "                   ('NN','RB'):0.0177,('NN','DT'):0.0068,\n",
    "                   ('RB','NNP'):0.0068,('RB','MD'):0.0102,('RB','VB'):0.1011,('RB','JJ'):0.1012,('RB','NN'):0.0120,\n",
    "                   ('RB','RB'):0.0728,('RB','DT'):0.0479,\n",
    "                   ('DT','NNP'):0.1147,('DT','MD'):0.0021,('DT','VB'):0.0002,('DT','JJ'):0.2157,('DT','NN'):0.4744,\n",
    "                   ('DT','RB'):0.0102,('DT','DT'):0.0017}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_emission_prob = {('Janet','NNP'):0.000032, ('will','MD'):0.308431,('will','VB'):0.000028,('will','NN'):0.0002,\n",
    "                     ('back','VB'):0.000672,('back','JJ'):0.00034,('back','NN'):0.000223,('back','RB'):0.010446,\n",
    "                     ('the','NNP'):0.000048,('the','DT'):0.506099,('bill','VB'):0.000028,('bill','NN'):0.002337}\n",
    "# assume the missing entries are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "Viterbi(State_trans_prob, Word_emission_prob, Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging with nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that utilizes the POS tagger in nltk to obtain the POS tags of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def posTag(sentence):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    result = nltk.pos_tag(words)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Janet', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('back', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('bill', 'NN')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "posTag(Sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
